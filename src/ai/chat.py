from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.prompts import PromptTemplate

# --------- Config ---------
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

PERSIST_DIR = "../chroma_db"
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "http://localhost:8080/v1")

# --------- Load vector store ---------
embeddings = OpenAIEmbeddings(
    model="embeddings",
    openai_api_base=OPENAI_API_BASE,
    openai_api_key="not-needed"
)

vs = Chroma(
    persist_directory=PERSIST_DIR,
    embedding_function=embeddings
)

# --------- Prompt √©p tr·∫£ l·ªùi Ti·∫øng Vi·ªát ---------
prompt_template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh.
    Lu√¥n lu√¥n tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT r√µ r√†ng, t·ª± nhi√™n, k·ªÉ c·∫£ khi c√¢u h·ªèi b·∫±ng ng√¥n ng·ªØ kh√°c.
    N·∫øu kh√¥ng bi·∫øt h√£y tr·∫£ l·ªùi: "Kh√¥ng c√≥ th√¥ng tin", kh√¥ng b·ªãa ra th√¥ng tin ho·∫∑c l·∫•y t·ª´ b√™n ngo√†i.

    Ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu:
    {context}

    C√¢u h·ªèi: {question}

    Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát:
"""

CUSTOM_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# --------- LLM ---------
llm = ChatOllama(model=LLM_MODEL)

# --------- Chat loop ---------
print("üí¨ Chat v·ªõi t√†i li·ªáu (embedding tr∆∞·ªõc khi tr·∫£ l·ªùi)\n")

queries = [
    "·ª®ng d·ª•ng c√≥ nh·ªØng danh m·ª•c n√†o?",
    "D·ªçn d·∫πp v·ªá sinh bao g·ªìm nh·ªØng d·ªãch v·ª• n√†o?",
    "ChƒÉm s√≥c s·ª©c kh·ªèe bao g·ªìm nh·ªØng d·ªãch v·ª• n√†o?",
    "D·ªãch v·ª• d·ªçn d·∫πp Ph√≤ng kh√°ch bao g·ªìm nh·ªØng c√¥ng vi·ªác g√¨?",
    "D·ªãch v·ª• chƒÉm s√≥c ng∆∞·ªùi khuy·∫øt t·∫≠t ph·∫£i l√†m nh·ªØng g√¨?",
    "D·ªãch v·ª• chƒÉm s√≥c ng∆∞·ªùi l·ªõn tu·ªïi kh√¥ng l√†m nh·ªØng g√¨?"
]

for query in queries:
    print("\nB·∫°n:", query)

    # --- 1. Embedding query ---
    query_vector = embeddings.embed_query(query)

    # --- 2. Search trong VectorDB ---
    docs = vs.similarity_search_by_vector(query_vector, k=4)
    context = "\n\n".join([d.page_content for d in docs])

    # --- 3. G·ª≠i v√†o LLM ---
    prompt = CUSTOM_PROMPT.format(context=context, question=query)
    response = llm.invoke(prompt)

    print("\nü§ñ Tr·∫£ l·ªùi:", response.content)
    print("\nüìö Ngu·ªìn:")
    for d in docs:
        print(f" - {d.metadata.get('source')} | chunk_id={d.metadata.get('chunk_id')}")
    print("-" * 50)
