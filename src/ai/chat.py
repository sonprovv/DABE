from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.prompts import PromptTemplate

import os
import sys
import json
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate

def debug_imports():
    print("\nDEBUG: Python Environment")
    print(f"Python Version: {sys.version}")
    print(f"Python Path: {sys.path}")
    print(f"Current Directory: {os.getcwd()}")
    print("\nDEBUG: Installed Packages")
    import pkg_resources
    installed_packages = [f"{dist.key} {dist.version}" for dist in pkg_resources.working_set]
    print("\n".join(installed_packages))

try:
    debug_imports()
    print("\nStarting initialization...")

    # Add the parent directory to Python path to fix imports
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    if parent_dir not in sys.path:
        sys.path.append(parent_dir)
        print(f"Added {parent_dir} to Python path")

    PERSIST_DIR = os.path.join(parent_dir, "chroma_db")
    print(f"Using Chroma DB directory: {PERSIST_DIR}")

    # --------- Load embeddings and vector store ---------
    print("Loading embeddings model...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    print("Embeddings model loaded successfully")

    print("Initializing Chroma vector store...")
    vs = Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings
    )
    print("Chroma vector store initialized successfully")

except Exception as e:
    print(f"Error during initialization: {str(e)}", file=sys.stderr)
    raise

# --------- Prompt √©p tr·∫£ l·ªùi Ti·∫øng Vi·ªát ---------
prompt_template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh.
    Lu√¥n lu√¥n tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT r√µ r√†ng, t·ª± nhi√™n, k·ªÉ c·∫£ khi c√¢u h·ªèi b·∫±ng ng√¥n ng·ªØ kh√°c.
    N·∫øu kh√¥ng bi·∫øt h√£y tr·∫£ l·ªùi: "Kh√¥ng c√≥ th√¥ng tin", kh√¥ng b·ªãa ra th√¥ng tin ho·∫∑c l·∫•y t·ª´ b√™n ngo√†i.

    Ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu:
    {context}

    C√¢u h·ªèi: {question}

    Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát:
"""

CUSTOM_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# --------- LLM ---------
llm = ChatOllama(model=LLM_MODEL)

# --------- Chat loop ---------
print("üí¨ Chat v·ªõi t√†i li·ªáu (embedding tr∆∞·ªõc khi tr·∫£ l·ªùi)\n")

queries = [
    "·ª®ng d·ª•ng c√≥ nh·ªØng danh m·ª•c n√†o?",
    "D·ªçn d·∫πp v·ªá sinh bao g·ªìm nh·ªØng d·ªãch v·ª• n√†o?",
    "ChƒÉm s√≥c s·ª©c kh·ªèe bao g·ªìm nh·ªØng d·ªãch v·ª• n√†o?",
    "D·ªãch v·ª• d·ªçn d·∫πp Ph√≤ng kh√°ch bao g·ªìm nh·ªØng c√¥ng vi·ªác g√¨?",
    "D·ªãch v·ª• chƒÉm s√≥c ng∆∞·ªùi khuy·∫øt t·∫≠t ph·∫£i l√†m nh·ªØng g√¨?",
    "D·ªãch v·ª• chƒÉm s√≥c ng∆∞·ªùi l·ªõn tu·ªïi kh√¥ng l√†m nh·ªØng g√¨?"
]

for query in queries:
    print("\nB·∫°n:", query)

    # --- 1. Embedding query ---
    query_vector = embeddings.embed_query(query)

    # --- 2. Search trong VectorDB ---
    docs = vs.similarity_search_by_vector(query_vector, k=4)
    context = "\n\n".join([d.page_content for d in docs])

    # --- 3. G·ª≠i v√†o LLM ---
    prompt = CUSTOM_PROMPT.format(context=context, question=query)
    response = llm.invoke(prompt)

    print("\nü§ñ Tr·∫£ l·ªùi:", response.content)
    print("\nüìö Ngu·ªìn:")
    for d in docs:
        print(f" - {d.metadata.get('source')} | chunk_id={d.metadata.get('chunk_id')}")
    print("-" * 50)
